{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion LLM Conversion Quick Start\n",
    "\n",
    "This notebook demonstrates basic usage of the diffusion conversion pipeline.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline converts pre-trained autoregressive language models (like GPT-2, LLaMA, etc.) into diffusion-based language models that can generate text through a denoising process.\n",
    "\n",
    "### Key Components:\n",
    "- **DiffusionTransformer**: Wrapper that converts causal attention to bidirectional\n",
    "- **MaskedDiffusionScheduler**: Handles noise addition and denoising schedule\n",
    "- **Multi-stage training**: Gradual conversion from autoregressive to diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from models.diffusion_transformer import DiffusionTransformer, DiffusionTransformerConfig\n",
    "from models.noise_scheduler import MaskedDiffusionScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Base Model\n",
    "\n",
    "We'll start with a small GPT-2 model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model for testing\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in base_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Mask token: {tokenizer.mask_token} (ID: {tokenizer.mask_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Diffusion Model\n",
    "\n",
    "Convert the autoregressive model to a diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diffusion config\n",
    "config = DiffusionTransformerConfig(\n",
    "    base_model_name=model_name,\n",
    "    hidden_size=base_model.config.hidden_size,\n",
    "    num_timesteps=100,  # Fewer timesteps for demo\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    use_bidirectional_attention=True,\n",
    "    freeze_base_model=True  # Start with frozen base model\n",
    ")\n",
    "\n",
    "# Create diffusion model\n",
    "diffusion_model = DiffusionTransformer(base_model, config)\n",
    "diffusion_model.to(device)\n",
    "\n",
    "print(f\"Diffusion model created with {sum(p.numel() for p in diffusion_model.parameters()) / 1e6:.1f}M parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in diffusion_model.parameters() if p.requires_grad) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Noise Scheduler\n",
    "\n",
    "The noise scheduler handles the diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise scheduler\n",
    "scheduler = MaskedDiffusionScheduler(\n",
    "    num_timesteps=config.num_timesteps,\n",
    "    schedule_type=\"cosine\",\n",
    "    mask_token_id=tokenizer.mask_token_id\n",
    ")\n",
    "\n",
    "print(f\"Scheduler created with {scheduler.num_timesteps} timesteps\")\n",
    "print(f\"Schedule type: {scheduler.schedule_type}\")\n",
    "\n",
    "# Visualize noise schedule\n",
    "timesteps = np.arange(scheduler.num_timesteps)\n",
    "noise_levels = [scheduler.get_noise_level(t) for t in timesteps]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(timesteps, noise_levels)\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Noise Level')\n",
    "plt.title('Noise Schedule')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Forward Pass\n",
    "\n",
    "Let's test the model with a simple forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple input\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "print(f\"Input text: {test_text}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
    "\n",
    "# Add noise at different timesteps\n",
    "timesteps = torch.tensor([0, 25, 50, 75], device=device)\n",
    "\n",
    "print(\"\\nNoise examples:\")\n",
    "for t in timesteps:\n",
    "    noisy_ids, mask = scheduler.add_noise(input_ids, t.unsqueeze(0))\n",
    "    masked_text = tokenizer.decode(noisy_ids[0], skip_special_tokens=False)\n",
    "    print(f\"t={t:2d}: {masked_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Through Diffusion Model\n",
    "\n",
    "Test the diffusion model's forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "diffusion_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test at different timesteps\n",
    "    for t in [10, 50, 90]:\n",
    "        timestep = torch.tensor([t], device=device)\n",
    "        \n",
    "        # Add noise\n",
    "        noisy_ids, mask = scheduler.add_noise(input_ids, timestep)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = diffusion_model(\n",
    "            input_ids=noisy_ids,\n",
    "            timesteps=timestep,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        logits = outputs['logits']\n",
    "        \n",
    "        print(f\"\\nTimestep {t}:\")\n",
    "        print(f\"  Input shape: {noisy_ids.shape}\")\n",
    "        print(f\"  Output shape: {logits.shape}\")\n",
    "        print(f\"  Masked positions: {mask.sum().item()}\")\n",
    "        \n",
    "        # Show prediction for first masked token\n",
    "        if mask.any():\n",
    "            first_mask_pos = mask[0].nonzero()[0].item()\n",
    "            pred_logits = logits[0, first_mask_pos]\n",
    "            pred_id = pred_logits.argmax().item()\n",
    "            pred_token = tokenizer.decode([pred_id])\n",
    "            original_token = tokenizer.decode([input_ids[0, first_mask_pos].item()])\n",
    "            print(f\"  Prediction for pos {first_mask_pos}: '{pred_token}' (original: '{original_token}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Bidirectional Attention\n",
    "\n",
    "Compare outputs with and without bidirectional attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bidirectional vs causal attention\n",
    "test_input = \"The cat sat on the [MASK] mat.\"\n",
    "test_tokens = tokenizer(test_input.replace('[MASK]', tokenizer.mask_token), \n",
    "                       return_tensors=\"pt\", padding=True)\n",
    "test_ids = test_tokens['input_ids'].to(device)\n",
    "test_mask = test_tokens['attention_mask'].to(device)\n",
    "\n",
    "print(f\"Test input: {test_input}\")\n",
    "print(f\"Tokenized: {tokenizer.convert_ids_to_tokens(test_ids[0])}\")\n",
    "\n",
    "# Find mask position\n",
    "mask_pos = (test_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "print(f\"Mask position: {mask_pos}\")\n",
    "\n",
    "timestep = torch.tensor([50], device=device)\n",
    "\n",
    "# Test with bidirectional attention\n",
    "diffusion_model.config.use_bidirectional_attention = True\n",
    "with torch.no_grad():\n",
    "    outputs_bi = diffusion_model(\n",
    "        input_ids=test_ids,\n",
    "        timesteps=timestep,\n",
    "        attention_mask=test_mask\n",
    "    )\n",
    "    \n",
    "    # Get top predictions\n",
    "    logits_bi = outputs_bi['logits'][0, mask_pos]\n",
    "    top_ids_bi = logits_bi.topk(5).indices\n",
    "    top_tokens_bi = [tokenizer.decode([id.item()]) for id in top_ids_bi]\n",
    "    \n",
    "print(f\"\\nBidirectional attention - Top 5 predictions:\")\n",
    "for i, token in enumerate(top_tokens_bi):\n",
    "    print(f\"  {i+1}. '{token}'\")\n",
    "\n",
    "# Test with causal attention\n",
    "diffusion_model.config.use_bidirectional_attention = False\n",
    "with torch.no_grad():\n",
    "    outputs_causal = diffusion_model(\n",
    "        input_ids=test_ids,\n",
    "        timesteps=timestep,\n",
    "        attention_mask=test_mask\n",
    "    )\n",
    "    \n",
    "    # Get top predictions\n",
    "    logits_causal = outputs_causal['logits'][0, mask_pos]\n",
    "    top_ids_causal = logits_causal.topk(5).indices\n",
    "    top_tokens_causal = [tokenizer.decode([id.item()]) for id in top_ids_causal]\n",
    "    \n",
    "print(f\"\\nCausal attention - Top 5 predictions:\")\n",
    "for i, token in enumerate(top_tokens_causal):\n",
    "    print(f\"  {i+1}. '{token}'\")\n",
    "\n",
    "# Reset to bidirectional\n",
    "diffusion_model.config.use_bidirectional_attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple Generation Test\n",
    "\n",
    "Test the generation capabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation test\n",
    "def simple_generate(model, scheduler, tokenizer, prompt, max_length=20, num_steps=10):\n",
    "    \"\"\"Simple generation using the diffusion model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    if prompt:\n",
    "        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "        prompt_len = prompt_tokens.shape[1]\n",
    "    else:\n",
    "        prompt_tokens = None\n",
    "        prompt_len = 0\n",
    "    \n",
    "    # Initialize with masks\n",
    "    seq_len = max_length\n",
    "    input_ids = torch.full((1, seq_len), tokenizer.mask_token_id, device=device)\n",
    "    \n",
    "    if prompt_tokens is not None:\n",
    "        input_ids[0, :prompt_len] = prompt_tokens[0]\n",
    "    \n",
    "    # Simple sampling loop\n",
    "    timesteps = torch.linspace(scheduler.num_timesteps-1, 0, num_steps, device=device).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in timesteps:\n",
    "            t_batch = t.unsqueeze(0)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(input_ids=input_ids, timesteps=t_batch)\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            # Sample from logits for masked positions\n",
    "            mask = (input_ids == tokenizer.mask_token_id)\n",
    "            if mask.any():\n",
    "                # Simple sampling - take top prediction\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                \n",
    "                # Only update some masked positions (gradual denoising)\n",
    "                masked_positions = mask.nonzero(as_tuple=False)\n",
    "                num_to_update = max(1, len(masked_positions) // (num_steps - timesteps.tolist().index(t)))\n",
    "                \n",
    "                if len(masked_positions) > 0:\n",
    "                    # Update random subset of masked positions\n",
    "                    update_indices = torch.randperm(len(masked_positions))[:num_to_update]\n",
    "                    for idx in update_indices:\n",
    "                        pos = masked_positions[idx]\n",
    "                        input_ids[pos[0], pos[1]] = predictions[pos[0], pos[1]]\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Test generation\n",
    "print(\"Testing generation...\")\n",
    "prompt = \"The weather today is\"\n",
    "generated = simple_generate(diffusion_model, scheduler, tokenizer, prompt, max_length=15, num_steps=5)\n",
    "generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "print(f\"Generated: '{generated_text}'\")\n",
    "\n",
    "# Test without prompt\n",
    "print(\"\\nTesting generation without prompt...\")\n",
    "generated_no_prompt = simple_generate(diffusion_model, scheduler, tokenizer, \"\", max_length=12, num_steps=8)\n",
    "generated_text_no_prompt = tokenizer.decode(generated_no_prompt[0], skip_special_tokens=True)\n",
    "print(f\"Generated: '{generated_text_no_prompt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Analysis\n",
    "\n",
    "Analyze the model's structure and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"  Base model: {diffusion_model.base_model.__class__.__name__}\")\n",
    "print(f\"  Hidden size: {diffusion_model.config.hidden_size}\")\n",
    "print(f\"  Vocab size: {diffusion_model.base_config.vocab_size}\")\n",
    "print(f\"  Timesteps: {diffusion_model.config.num_timesteps}\")\n",
    "print(f\"  Bidirectional: {diffusion_model.config.use_bidirectional_attention}\")\n",
    "print(f\"  Frozen base: {diffusion_model.config.freeze_base_model}\")\n",
    "\n",
    "# Count parameters by component\n",
    "base_params = sum(p.numel() for p in diffusion_model.base_model.parameters())\n",
    "time_params = sum(p.numel() for p in diffusion_model.time_embed.parameters()) + \\\n",
    "              sum(p.numel() for p in diffusion_model.time_mlp.parameters())\n",
    "head_params = sum(p.numel() for p in diffusion_model.denoising_head.parameters())\n",
    "\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  Base model: {base_params / 1e6:.1f}M\")\n",
    "print(f\"  Time components: {time_params / 1e6:.1f}M\")\n",
    "print(f\"  Denoising head: {head_params / 1e6:.1f}M\")\n",
    "print(f\"  Total: {(base_params + time_params + head_params) / 1e6:.1f}M\")\n",
    "\n",
    "# Memory usage estimate\n",
    "model_size_mb = (base_params + time_params + head_params) * 4 / 1e6  # 4 bytes per float32\n",
    "print(f\"\\nMemory estimate: {model_size_mb:.1f}MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "This notebook demonstrates the basic setup and testing of the diffusion model. For actual training:\n",
    "\n",
    "1. **Use the training script**: `python scripts/train_diffusion.py --model_config configs/model_configs/gpt2_diffusion.yaml --training_config configs/training_configs/default.yaml --hardware_config configs/hardware_configs/gpu.yaml`\n",
    "\n",
    "2. **Try different models**: The pipeline supports various architectures (GPT-2, GPT-Neo, LLaMA, etc.)\n",
    "\n",
    "3. **Experiment with hyperparameters**: Adjust timesteps, noise schedules, training stages\n",
    "\n",
    "4. **Evaluate on benchmarks**: Test perplexity, generation quality, etc.\n",
    "\n",
    "5. **Fine-tune for specific tasks**: Adapt the model for particular applications\n",
    "\n",
    "The conversion from autoregressive to diffusion models opens up new possibilities for text generation and understanding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}